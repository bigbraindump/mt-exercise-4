2024-05-07 07:04:48,273 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-07 07:04:48,274 - INFO - joeynmt.helpers -                           cfg.name : pre_norm
2024-05-07 07:04:48,274 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2024-05-07 07:04:48,274 - INFO - joeynmt.helpers -                     cfg.data.train : test/data/toy/train
2024-05-07 07:04:48,274 - INFO - joeynmt.helpers -                       cfg.data.dev : test/data/toy/dev
2024-05-07 07:04:48,274 - INFO - joeynmt.helpers -                      cfg.data.test : test/data/toy/test
2024-05-07 07:04:48,274 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2024-05-07 07:04:48,275 - INFO - joeynmt.helpers -       cfg.data.sample_train_subset : -1
2024-05-07 07:04:48,275 - INFO - joeynmt.helpers -         cfg.data.sample_dev_subset : -1
2024-05-07 07:04:48,275 - INFO - joeynmt.helpers -                  cfg.data.src.lang : de
2024-05-07 07:04:48,275 - INFO - joeynmt.helpers -            cfg.data.src.max_length : 30
2024-05-07 07:04:48,275 - INFO - joeynmt.helpers -            cfg.data.src.min_length : 1
2024-05-07 07:04:48,275 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2024-05-07 07:04:48,275 - INFO - joeynmt.helpers -             cfg.data.src.normalize : False
2024-05-07 07:04:48,275 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2024-05-07 07:04:48,275 - INFO - joeynmt.helpers -             cfg.data.src.voc_limit : 400
2024-05-07 07:04:48,275 - INFO - joeynmt.helpers -          cfg.data.src.voc_min_freq : 1
2024-05-07 07:04:48,275 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : test/data/toy/bpe200.txt
2024-05-07 07:04:48,275 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2024-05-07 07:04:48,276 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.num_merges : 200
2024-05-07 07:04:48,276 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : test/data/toy/bpe200.codes
2024-05-07 07:04:48,276 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.dropout : 0.0
2024-05-07 07:04:48,276 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : none
2024-05-07 07:04:48,276 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : en
2024-05-07 07:04:48,276 - INFO - joeynmt.helpers -            cfg.data.trg.max_length : 30
2024-05-07 07:04:48,276 - INFO - joeynmt.helpers -            cfg.data.trg.min_length : 1
2024-05-07 07:04:48,276 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2024-05-07 07:04:48,276 - INFO - joeynmt.helpers -             cfg.data.trg.normalize : False
2024-05-07 07:04:48,276 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2024-05-07 07:04:48,276 - INFO - joeynmt.helpers -             cfg.data.trg.voc_limit : 400
2024-05-07 07:04:48,276 - INFO - joeynmt.helpers -          cfg.data.trg.voc_min_freq : 1
2024-05-07 07:04:48,277 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : test/data/toy/bpe200.txt
2024-05-07 07:04:48,277 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2024-05-07 07:04:48,277 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.num_merges : 200
2024-05-07 07:04:48,277 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : test/data/toy/bpe200.codes
2024-05-07 07:04:48,277 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.dropout : 0.0
2024-05-07 07:04:48,277 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : none
2024-05-07 07:04:48,277 - INFO - joeynmt.helpers -                 cfg.testing.n_best : 1
2024-05-07 07:04:48,277 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2024-05-07 07:04:48,277 - INFO - joeynmt.helpers -             cfg.testing.beam_alpha : 1.0
2024-05-07 07:04:48,277 - INFO - joeynmt.helpers -             cfg.testing.batch_size : 10
2024-05-07 07:04:48,277 - INFO - joeynmt.helpers -             cfg.testing.batch_type : sentence
2024-05-07 07:04:48,277 - INFO - joeynmt.helpers -           cfg.testing.eval_metrics : ['bleu']
2024-05-07 07:04:48,278 - INFO - joeynmt.helpers -      cfg.testing.max_output_length : 31
2024-05-07 07:04:48,278 - INFO - joeynmt.helpers -      cfg.testing.min_output_length : 1
2024-05-07 07:04:48,278 - INFO - joeynmt.helpers -            cfg.testing.return_prob : none
2024-05-07 07:04:48,278 - INFO - joeynmt.helpers -       cfg.testing.return_attention : False
2024-05-07 07:04:48,278 - INFO - joeynmt.helpers -           cfg.testing.generate_unk : True
2024-05-07 07:04:48,278 - INFO - joeynmt.helpers -   cfg.testing.no_repeat_ngram_size : -1
2024-05-07 07:04:48,278 - INFO - joeynmt.helpers -     cfg.testing.repetition_penalty : -1
2024-05-07 07:04:48,278 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.whitespace : False
2024-05-07 07:04:48,278 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.tokenize : 13a
2024-05-07 07:04:48,278 - INFO - joeynmt.helpers -       cfg.training.reset_best_ckpt : False
2024-05-07 07:04:48,278 - INFO - joeynmt.helpers -       cfg.training.reset_scheduler : False
2024-05-07 07:04:48,278 - INFO - joeynmt.helpers -       cfg.training.reset_optimizer : False
2024-05-07 07:04:48,279 - INFO - joeynmt.helpers -      cfg.training.reset_iter_state : False
2024-05-07 07:04:48,279 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2024-05-07 07:04:48,279 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2024-05-07 07:04:48,279 - INFO - joeynmt.helpers -            cfg.training.adam_betas : [0.9, 0.999]
2024-05-07 07:04:48,279 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.005
2024-05-07 07:04:48,279 - INFO - joeynmt.helpers -     cfg.training.learning_rate_min : 0.0001
2024-05-07 07:04:48,279 - INFO - joeynmt.helpers -         cfg.training.clip_grad_val : 1.0
2024-05-07 07:04:48,279 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2024-05-07 07:04:48,279 - INFO - joeynmt.helpers -                  cfg.training.loss : crossentropy
2024-05-07 07:04:48,279 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.0
2024-05-07 07:04:48,279 - INFO - joeynmt.helpers -            cfg.training.batch_size : 10
2024-05-07 07:04:48,279 - INFO - joeynmt.helpers -            cfg.training.batch_type : sentence
2024-05-07 07:04:48,280 - INFO - joeynmt.helpers -      cfg.training.batch_multiplier : 1
2024-05-07 07:04:48,280 - INFO - joeynmt.helpers -         cfg.training.normalization : batch
2024-05-07 07:04:48,280 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2024-05-07 07:04:48,280 - INFO - joeynmt.helpers -              cfg.training.patience : 5
2024-05-07 07:04:48,280 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.5
2024-05-07 07:04:48,280 - INFO - joeynmt.helpers -                cfg.training.epochs : 1
2024-05-07 07:04:48,280 - INFO - joeynmt.helpers -               cfg.training.updates : 100
2024-05-07 07:04:48,280 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 10
2024-05-07 07:04:48,280 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 10
2024-05-07 07:04:48,280 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : loss
2024-05-07 07:04:48,280 - INFO - joeynmt.helpers -             cfg.training.model_dir : pre_norm
2024-05-07 07:04:48,280 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2024-05-07 07:04:48,280 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2024-05-07 07:04:48,281 - INFO - joeynmt.helpers -              cfg.training.use_cuda : False
2024-05-07 07:04:48,281 - INFO - joeynmt.helpers -                  cfg.training.fp16 : True
2024-05-07 07:04:48,281 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2]
2024-05-07 07:04:48,281 - INFO - joeynmt.helpers -       cfg.training.keep_best_ckpts : 3
2024-05-07 07:04:48,281 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2024-05-07 07:04:48,281 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2024-05-07 07:04:48,281 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2024-05-07 07:04:48,281 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2024-05-07 07:04:48,281 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2024-05-07 07:04:48,281 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : False
2024-05-07 07:04:48,281 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2024-05-07 07:04:48,281 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2024-05-07 07:04:48,282 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 3
2024-05-07 07:04:48,282 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 4
2024-05-07 07:04:48,282 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 64
2024-05-07 07:04:48,282 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2024-05-07 07:04:48,282 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.freeze : False
2024-05-07 07:04:48,282 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 64
2024-05-07 07:04:48,282 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 128
2024-05-07 07:04:48,282 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0.1
2024-05-07 07:04:48,282 - INFO - joeynmt.helpers -           cfg.model.encoder.freeze : False
2024-05-07 07:04:48,282 - INFO - joeynmt.helpers -       cfg.model.encoder.layer_norm : pre
2024-05-07 07:04:48,282 - INFO - joeynmt.helpers -       cfg.model.encoder.activation : relu
2024-05-07 07:04:48,282 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2024-05-07 07:04:48,283 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 3
2024-05-07 07:04:48,283 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 4
2024-05-07 07:04:48,283 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 64
2024-05-07 07:04:48,283 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2024-05-07 07:04:48,283 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.freeze : False
2024-05-07 07:04:48,283 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 64
2024-05-07 07:04:48,283 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 128
2024-05-07 07:04:48,283 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0.1
2024-05-07 07:04:48,283 - INFO - joeynmt.helpers -           cfg.model.decoder.freeze : False
2024-05-07 07:04:48,283 - INFO - joeynmt.helpers -       cfg.model.decoder.layer_norm : pre
2024-05-07 07:04:48,283 - INFO - joeynmt.helpers -       cfg.model.decoder.activation : relu
2024-05-07 07:04:48,444 - INFO - joeynmt.data - Building tokenizer...
2024-05-07 07:04:48,461 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(1, 30), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-07 07:04:48,461 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(1, 30), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-07 07:04:48,461 - INFO - joeynmt.data - Loading train set...
2024-05-07 07:04:48,473 - INFO - joeynmt.data - Building vocabulary...
2024-05-07 07:04:48,478 - INFO - joeynmt.data - Loading dev set...
2024-05-07 07:04:48,496 - INFO - joeynmt.data - Loading test set...
2024-05-07 07:04:48,528 - INFO - joeynmt.data - Data loaded.
2024-05-07 07:04:48,528 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=1000, src_lang=de, trg_lang=en, has_trg=True, random_subset=-1)
2024-05-07 07:04:48,529 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=20, src_lang=de, trg_lang=en, has_trg=True, random_subset=-1)
2024-05-07 07:04:48,529 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=20, src_lang=de, trg_lang=en, has_trg=False, random_subset=-1)
2024-05-07 07:04:48,529 - INFO - joeynmt.data - First training example:
	[SRC] D@@ a@@ v@@ i@@ d G@@ all@@ o@@ : D@@ as ist B@@ il@@ l L@@ an@@ ge@@ . I@@ ch b@@ in D@@ a@@ ve G@@ all@@ o@@ .
	[TRG] D@@ a@@ v@@ i@@ d G@@ all@@ o@@ : Th@@ is is B@@ il@@ l L@@ an@@ ge@@ . I@@ '@@ m D@@ a@@ ve G@@ all@@ o@@ .
2024-05-07 07:04:48,530 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) t@@ (5) s@@ (6) e (7) e@@ (8) d@@ (9) o@@
2024-05-07 07:04:48,530 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) t@@ (5) s@@ (6) e (7) e@@ (8) d@@ (9) o@@
2024-05-07 07:04:48,530 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 204
2024-05-07 07:04:48,530 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 204
2024-05-07 07:04:48,531 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-07 07:04:48,549 - INFO - joeynmt.model - Enc-dec model built.
2024-05-07 07:04:48,553 - INFO - joeynmt.model - Total params: 277504
2024-05-07 07:04:48,553 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2024-05-07 07:04:48,553 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=3, num_heads=4, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=3, num_heads=4, alpha=1.0, layer_norm="pre", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=64, vocab_size=204),
	trg_embed=Embeddings(embedding_dim=64, vocab_size=204),
	loss_function=XentLoss(criterion=NLLLoss(), smoothing=0.0))
2024-05-07 07:04:49,567 - INFO - joeynmt.builders - Adam(lr=0.005, weight_decay=0.0, betas=[0.9, 0.999])
2024-05-07 07:04:49,568 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.5, patience=5)
2024-05-07 07:04:49,569 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: True
	gradient accumulation: 1
	batch size per device: 10
	effective batch size (w. parallel & accumulation): 10
2024-05-07 07:04:49,569 - INFO - joeynmt.training - EPOCH 1
2024-05-07 07:04:50,869 - INFO - joeynmt.training - Epoch   1, Step:       10, Batch Loss:    87.683975, Batch Acc: 0.041280, Tokens per Sec:     1491, Lr: 0.005000
2024-05-07 07:04:50,870 - INFO - joeynmt.prediction - Predicting 20 example(s)... (Greedy decoding with min_output_length=1, max_output_length=31, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-07 07:04:55,573 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.2
2024-05-07 07:04:55,573 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   0.00, loss: 262.12, ppl: 139.26, acc:   0.02, generation: 4.6983[sec], evaluation: 0.0040[sec]
2024-05-07 07:04:55,574 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2024-05-07 07:04:55,602 - INFO - joeynmt.training - Example #0
2024-05-07 07:04:55,602 - DEBUG - joeynmt.training - 	Tokenized source:     ['I@@', 'ch', 'f@@', 're@@', 'u@@', 'e', 'm@@', 'ich', ',', 'd@@', 'as@@', 's', 'ich', 'd@@', 'a', 'b@@', 'in', '.']
2024-05-07 07:04:55,602 - DEBUG - joeynmt.training - 	Tokenized reference:  ['I@@', '’@@', 'm', 'ha@@', 'p@@', 'p@@', 'y', 'to', 'b@@', 'e', 'h@@', 'ere', '.']
2024-05-07 07:04:55,602 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '</s>']
2024-05-07 07:04:55,602 - INFO - joeynmt.training - 	Source:     Ich freue mich , dass ich da bin .
2024-05-07 07:04:55,603 - INFO - joeynmt.training - 	Reference:  I’m happy to be here .
2024-05-07 07:04:55,603 - INFO - joeynmt.training - 	Hypothesis: e e e e e e e e e e e e e e e
2024-05-07 07:04:55,603 - INFO - joeynmt.training - Example #1
2024-05-07 07:04:55,603 - DEBUG - joeynmt.training - 	Tokenized source:     ['J@@', 'a', ',', 'g@@', 'u@@', 'ten', 'T@@', 'a@@', 'g', '.']
2024-05-07 07:04:55,603 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Y@@', 'es', ',', 'h@@', 'ell@@', 'o', '.']
2024-05-07 07:04:55,603 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '</s>']
2024-05-07 07:04:55,603 - INFO - joeynmt.training - 	Source:     Ja , guten Tag .
2024-05-07 07:04:55,603 - INFO - joeynmt.training - 	Reference:  Yes , hello .
2024-05-07 07:04:55,603 - INFO - joeynmt.training - 	Hypothesis: e e e e e e e e e e e e e e
2024-05-07 07:04:55,603 - INFO - joeynmt.training - Example #2
2024-05-07 07:04:55,603 - DEBUG - joeynmt.training - 	Tokenized source:     ['J@@', 'a', ',', 'al@@', 'so', ',', 'was', 's@@', 'ol@@', 'l', 'B@@', 'i@@', 'o@@', 'ha@@', 'c@@', 'k@@', 'ing', 's@@', 'ein', '?']
2024-05-07 07:04:55,603 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Y@@', 'es', ',', 'so', ',', 'w@@', 'hat', 'is', 'b@@', 'i@@', 'o@@', 'ha@@', 'c@@', 'k@@', 'ing', '?']
2024-05-07 07:04:55,603 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '</s>']
2024-05-07 07:04:55,603 - INFO - joeynmt.training - 	Source:     Ja , also , was soll Biohacking sein ?
2024-05-07 07:04:55,603 - INFO - joeynmt.training - 	Reference:  Yes , so , what is biohacking ?
2024-05-07 07:04:55,603 - INFO - joeynmt.training - 	Hypothesis: e e e e e e e e e e e e e e e e e e e
2024-05-07 07:04:56,791 - INFO - joeynmt.training - Epoch   1, Step:       20, Batch Loss:    99.463104, Batch Acc: 0.062944, Tokens per Sec:     1618, Lr: 0.005000
2024-05-07 07:04:56,792 - INFO - joeynmt.prediction - Predicting 20 example(s)... (Greedy decoding with min_output_length=1, max_output_length=31, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-07 07:04:59,151 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.2
2024-05-07 07:04:59,151 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   0.00, loss: 258.28, ppl: 129.54, acc:   0.03, generation: 2.3564[sec], evaluation: 0.0019[sec]
2024-05-07 07:04:59,152 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2024-05-07 07:04:59,178 - INFO - joeynmt.training - Example #0
2024-05-07 07:04:59,178 - DEBUG - joeynmt.training - 	Tokenized source:     ['I@@', 'ch', 'f@@', 're@@', 'u@@', 'e', 'm@@', 'ich', ',', 'd@@', 'as@@', 's', 'ich', 'd@@', 'a', 'b@@', 'in', '.']
2024-05-07 07:04:59,178 - DEBUG - joeynmt.training - 	Tokenized reference:  ['I@@', '’@@', 'm', 'ha@@', 'p@@', 'p@@', 'y', 'to', 'b@@', 'e', 'h@@', 'ere', '.']
2024-05-07 07:04:59,178 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Th@@', 'Th@@', 'c@@', 'c@@', 'c@@', 'c@@', 'c@@', 'c@@', 'c@@', 'c@@', 'c@@', 'c@@', 'c@@', '</s>']
2024-05-07 07:04:59,178 - INFO - joeynmt.training - 	Source:     Ich freue mich , dass ich da bin .
2024-05-07 07:04:59,178 - INFO - joeynmt.training - 	Reference:  I’m happy to be here .
2024-05-07 07:04:59,178 - INFO - joeynmt.training - 	Hypothesis: ThThccccccccccc
2024-05-07 07:04:59,179 - INFO - joeynmt.training - Example #1
2024-05-07 07:04:59,179 - DEBUG - joeynmt.training - 	Tokenized source:     ['J@@', 'a', ',', 'g@@', 'u@@', 'ten', 'T@@', 'a@@', 'g', '.']
2024-05-07 07:04:59,179 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Y@@', 'es', ',', 'h@@', 'ell@@', 'o', '.']
2024-05-07 07:04:59,179 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Th@@', 'Th@@', 'c@@', 'c@@', 'c@@', 'c@@', 'c@@', 'c@@', 'c@@', 'c@@', 'c@@', 'c@@', 'c@@', '</s>']
2024-05-07 07:04:59,179 - INFO - joeynmt.training - 	Source:     Ja , guten Tag .
2024-05-07 07:04:59,179 - INFO - joeynmt.training - 	Reference:  Yes , hello .
2024-05-07 07:04:59,179 - INFO - joeynmt.training - 	Hypothesis: ThThccccccccccc
2024-05-07 07:04:59,179 - INFO - joeynmt.training - Example #2
2024-05-07 07:04:59,179 - DEBUG - joeynmt.training - 	Tokenized source:     ['J@@', 'a', ',', 'al@@', 'so', ',', 'was', 's@@', 'ol@@', 'l', 'B@@', 'i@@', 'o@@', 'ha@@', 'c@@', 'k@@', 'ing', 's@@', 'ein', '?']
2024-05-07 07:04:59,179 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Y@@', 'es', ',', 'so', ',', 'w@@', 'hat', 'is', 'b@@', 'i@@', 'o@@', 'ha@@', 'c@@', 'k@@', 'ing', '?']
2024-05-07 07:04:59,179 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Th@@', 'Th@@', 'c@@', 'c@@', 'c@@', 'c@@', 'c@@', 'c@@', 'c@@', 'c@@', 'c@@', 'c@@', 'c@@', '</s>']
2024-05-07 07:04:59,179 - INFO - joeynmt.training - 	Source:     Ja , also , was soll Biohacking sein ?
2024-05-07 07:04:59,179 - INFO - joeynmt.training - 	Reference:  Yes , so , what is biohacking ?
2024-05-07 07:04:59,179 - INFO - joeynmt.training - 	Hypothesis: ThThccccccccccc
2024-05-07 07:05:00,386 - INFO - joeynmt.training - Epoch   1, Step:       30, Batch Loss:    92.879929, Batch Acc: 0.072063, Tokens per Sec:     1551, Lr: 0.005000
2024-05-07 07:05:00,387 - INFO - joeynmt.prediction - Predicting 20 example(s)... (Greedy decoding with min_output_length=1, max_output_length=31, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-07 07:05:04,733 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.2
2024-05-07 07:05:04,733 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   0.11, loss: 261.65, ppl: 138.04, acc:   0.04, generation: 4.3430[sec], evaluation: 0.0023[sec]
2024-05-07 07:05:04,761 - INFO - joeynmt.training - Example #0
2024-05-07 07:05:04,761 - DEBUG - joeynmt.training - 	Tokenized source:     ['I@@', 'ch', 'f@@', 're@@', 'u@@', 'e', 'm@@', 'ich', ',', 'd@@', 'as@@', 's', 'ich', 'd@@', 'a', 'b@@', 'in', '.']
2024-05-07 07:05:04,761 - DEBUG - joeynmt.training - 	Tokenized reference:  ['I@@', '’@@', 'm', 'ha@@', 'p@@', 'p@@', 'y', 'to', 'b@@', 'e', 'h@@', 'ere', '.']
2024-05-07 07:05:04,761 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I@@', '<unk>', '<unk>', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@']
2024-05-07 07:05:04,761 - INFO - joeynmt.training - 	Source:     Ich freue mich , dass ich da bin .
2024-05-07 07:05:04,761 - INFO - joeynmt.training - 	Reference:  I’m happy to be here .
2024-05-07 07:05:04,761 - INFO - joeynmt.training - 	Hypothesis: I<unk> <unk> oooooooooooooooooooooooooooo
2024-05-07 07:05:04,761 - INFO - joeynmt.training - Example #1
2024-05-07 07:05:04,761 - DEBUG - joeynmt.training - 	Tokenized source:     ['J@@', 'a', ',', 'g@@', 'u@@', 'ten', 'T@@', 'a@@', 'g', '.']
2024-05-07 07:05:04,761 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Y@@', 'es', ',', 'h@@', 'ell@@', 'o', '.']
2024-05-07 07:05:04,761 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I@@', '<unk>', '<unk>', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@']
2024-05-07 07:05:04,761 - INFO - joeynmt.training - 	Source:     Ja , guten Tag .
2024-05-07 07:05:04,761 - INFO - joeynmt.training - 	Reference:  Yes , hello .
2024-05-07 07:05:04,761 - INFO - joeynmt.training - 	Hypothesis: I<unk> <unk> oooooooooooooooooooooooooooo
2024-05-07 07:05:04,762 - INFO - joeynmt.training - Example #2
2024-05-07 07:05:04,762 - DEBUG - joeynmt.training - 	Tokenized source:     ['J@@', 'a', ',', 'al@@', 'so', ',', 'was', 's@@', 'ol@@', 'l', 'B@@', 'i@@', 'o@@', 'ha@@', 'c@@', 'k@@', 'ing', 's@@', 'ein', '?']
2024-05-07 07:05:04,762 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Y@@', 'es', ',', 'so', ',', 'w@@', 'hat', 'is', 'b@@', 'i@@', 'o@@', 'ha@@', 'c@@', 'k@@', 'ing', '?']
2024-05-07 07:05:04,762 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I@@', '<unk>', '<unk>', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@', 'o@@']
2024-05-07 07:05:04,762 - INFO - joeynmt.training - 	Source:     Ja , also , was soll Biohacking sein ?
2024-05-07 07:05:04,762 - INFO - joeynmt.training - 	Reference:  Yes , so , what is biohacking ?
2024-05-07 07:05:04,762 - INFO - joeynmt.training - 	Hypothesis: I<unk> <unk> oooooooooooooooooooooooooooo
2024-05-07 07:05:04,964 - INFO - joeynmt.training - Epoch   1: total training loss 2949.46
2024-05-07 07:05:04,964 - INFO - joeynmt.training - Training ended after   1 epochs.
2024-05-07 07:05:04,964 - INFO - joeynmt.training - Best validation result (greedy) at step       20: 258.28 loss.
2024-05-07 07:05:04,985 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-07 07:05:04,995 - INFO - joeynmt.model - Enc-dec model built.
2024-05-07 07:05:05,018 - INFO - joeynmt.helpers - Load model from /scratch/stariq/stariq/pre_norm/20.ckpt.
2024-05-07 07:05:05,022 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=3, num_heads=4, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=3, num_heads=4, alpha=1.0, layer_norm="pre", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=64, vocab_size=204),
	trg_embed=Embeddings(embedding_dim=64, vocab_size=204),
	loss_function=None)
2024-05-07 07:05:05,023 - INFO - joeynmt.prediction - Decoding on dev set...
2024-05-07 07:05:05,023 - INFO - joeynmt.prediction - Predicting 20 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=31, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-07 07:05:13,533 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.2
2024-05-07 07:05:13,533 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:   0.00, generation: 8.5058[sec], evaluation: 0.0033[sec]
2024-05-07 07:05:13,534 - INFO - joeynmt.prediction - Translations saved to: /scratch/stariq/stariq/pre_norm/00000020.hyps.dev.
2024-05-07 07:05:13,534 - INFO - joeynmt.prediction - Decoding on test set...
2024-05-07 07:05:13,534 - INFO - joeynmt.prediction - Predicting 20 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=31, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-07 07:05:18,590 - INFO - joeynmt.prediction - Generation took 5.0544[sec]. (No references given)
2024-05-07 07:05:18,592 - INFO - joeynmt.prediction - Translations saved to: /scratch/stariq/stariq/pre_norm/00000020.hyps.test.
